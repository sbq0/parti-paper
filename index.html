<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="main.tex"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
<div class="maketitle">_______________________________________________________________________________________________________________________________________________________________

<h2 class="titleHead">Scaling Autoregressive Models for Content-Rich
Text-to-Image Generation</h2>________________________________________________
 <div class="author" > <span 
class="ptmb8t-">Jiahui Yu</span><span class="footnote-mark">*</span><span 
class="ptmb8t-">&#x00A0;      Yuanzhong Xu</span><span class="footnote-mark">&#8224;</span><span 
class="ptmb8t-">&#x00A0;      Jing Yu Koh</span><span class="footnote-mark">&#8224;</span><span 
class="ptmb8t-">&#x00A0;      Thang Luong</span><span class="footnote-mark">&#8224;</span><span 
class="ptmb8t-">&#x00A0;      Gunjan Baid</span><span class="footnote-mark">&#8224;</span><br />
 <span 
class="ptmb8t-">Zirui Wang</span><span class="footnote-mark">&#8224;</span><span 
class="ptmb8t-">&#x00A0;         Vijay Vasudevan</span><span class="footnote-mark">&#8224;</span><span 
class="ptmb8t-">&#x00A0;         Alexander Ku</span><span class="footnote-mark">&#8224;</span><br />
 <span 
class="ptmb8t-">Yinfei Yang   Burcu Karagol Ayan   Ben Hutchinson</span><br />
 <span 
class="ptmb8t-">Wei Han   Zarana Parekh   Xin Li   Han Zhang</span><br />
 <span 
class="ptmb8t-">Jason Baldridge</span><span class="footnote-mark">&#8224;</span><span 
class="ptmb8t-">&#x00A0;</span><span 
class="ptmb8t-">&#x00A0;          Yonghui Wu</span><span class="footnote-mark">*</span><br />
 <span 
class="ectt-0900">{jiahuiyu, yuanzx, jykoh, thangluong, gunjanbaid, ziruiw, vrv, alexku,</span><br />
 <span 
class="ectt-0900">jasonbaldridge, yonghui}@google.com</span><br />
<br />  <span class="footnote-mark">*</span> &#x00A0;&#x00A0;Equal contribution.&#x00A0;&#x00A0;            <span class="footnote-mark">&#8224;</span> &#x00A0;&#x00A0;Core contribution.<br />
<br />   Google Research<br /><br />                                                    </div>
<br />
<div class="thanks" ><br /><a 
 id="tk-4"></a><span class="thank-mark">§</span>Correspondence to <span 
class="ectt-0900">{jiahuiyu, jasonbaldridge, yonghui}@google.com.</span></div>
<hr class="float"><div class="float" 
>
<span 
class="ptmr8t-x-x-90">Preprint.</span>
</div><hr class="endfloat" />
</div>
<hr class="figure"><div class="figure" 
>
                                                                                                                                                                                        
                                                                                                                                                                                        
                                                                                                                                                                                        
                                                                                                                                                                                        
       <img 
src="figures/teaser.jpg" alt="PIC"  
width="377" height="377" > <a 
 id="x1-2r1"></a>
       <a 
 id="x1-3"></a>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">Example images generated by Parti. <span 
class="ptmb8t-">Top row</span>: &#8220;<span 
class="ptmri8t-">Oil-on-canvas painting of a blue night sky</span>
<span 
class="ptmri8t-">with roiling energy. A fuzzy and bright yellow crescent moon shining at the top. Below the exploding</span>
<span 
class="ptmri8t-">yellow stars and radiating swirls of blue, a distant village sits quietly on the right. Connecting earth</span>
<span 
class="ptmri8t-">and sky is a flame-like cypress tree with curling and swaying branches on the left. A church spire</span>
<span 
class="ptmri8t-">rises as a beacon over rolling blue hills</span>.&#8221; (a 67-word description of the Starry Night by Vincent
van Gogh). <span 
class="ptmb8t-">Middle row</span>: &#8220;<span 
class="ptmri8t-">A close-up high-contrast photo of Sydney Opera House sitting next to</span>
<span 
class="ptmri8t-">Eiffel tower, under a blue night sky of roiling energy, exploding yellow stars, and radiating swirls of</span>
<span 
class="ptmri8t-">blue</span>&#8221;. <span 
class="ptmb8t-">Last row</span>: Similar to the middle row, but with &#8220;<span 
class="ptmri8t-">anime illustration</span>&#8221; and different landmarks
(<span 
class="ptmri8t-">the Great Pyramid and the Parthenon</span>).</span></div><!--tex4ht:label?: x1-2r -->
                                                                                                                                                                      
                                                                                                                                                                      
       </div><hr class="endfigure">
       <div 
class="abstract" 
>
       <div  
class="centerline">                                                                       <span 
class="ptmb8t-x-x-120">Abstract</span>                                                           </div>
              <div class="quote">
              We present the Pathways&#x00A0;[<span 
class="ptmb8t-">? </span>] Autoregressive Text-to-Image (Parti) model, which
              generates high-fidelity photorealistic images and supports content-rich synthesis
              involving complex compositions and world knowledge. Parti treats text-to-image
              generation   as   a   sequence-to-sequence   modeling   problem,   akin   to   machine
              translation, with sequences of image tokens as the target outputs rather than text
              tokens in another language. This strategy can naturally tap into the rich body of
              prior  work  on  large  language  models,  which  have  seen  continued  advances  in
              capabilities and performance through scaling data and model sizes. Our approach
              is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN,
              to encode images as sequences of discrete tokens. Second, we achieve consistent
              quality improvements by scaling the encoder-decoder Transformer model up to
              20B  parameters,  with  a  new  state-of-the-art  zero-shot  FID  score  of  7.23  and
              finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized
              Narratives as well as PartiPrompts (P<span 
class="cmr-10">2</span>), a new holistic benchmark of over 1600
              English  prompts,  demonstrate  the  effectiveness  of  Parti  across  a  wide  variety
              of  categories  and  difficulty  aspects.  We  also  explore  and  highlight  limitations
              of  our  models  in  order  to  define  and  exemplify  key  areas  of  focus  for  further
              improvements. See <a 
href="https://parti.research.google/" ><span 
class="ectt-1000">parti.research.google</span></a> for high-resolution images.
              </div>
       </div>
       <hr class="figure"><div class="figure" 
>
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
<dl class="list1"><dt class="list">
</dt><dd 
class="list">
                                 <div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/frog.jpg" alt="PIC"  
width="158" height="159" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/anubis_la.jpg" alt="PIC"  
width="158" height="159" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/panda.jpg" alt="PIC"  
width="158" height="159" ></div>                                                                                                                                                                          
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">A</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">A photo  of  a  frog  reading  the  newspaper  named  &#8220;Toaday&#8221;</span>
<span 
class="ptmri8t-x-x-60">written on it. There is a frog printed on the newspaper too.</span></div>
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">B</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">A portrait  of  a  statue  of  the  Egyptian  god  Anubis  wearing</span>
<span 
class="ptmri8t-x-x-60">aviator goggles, white t-shirt and leather jacket. The city of Los</span>
<span 
class="ptmri8t-x-x-60">Angeles is in the background. Hi-res DSLR photograph.</span></div>
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">C</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">A high-contrast photo of a panda riding a horse. The panda is</span>
<span 
class="ptmri8t-x-x-60">wearing a wizard hat and is reading a book. The horse is standing</span>
<span 
class="ptmri8t-x-x-60">on a street against a gray concrete wall. Colorful flowers and the</span>
<span 
class="ptmri8t-x-x-60">word "PEACE" are painted on the wall. Green grass grows from</span>
<span 
class="ptmri8t-x-x-60">cracks in the street. DSLR photograph. daytime lighting.</span></div>                                                  <br />
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/snake_salad.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/snake_pancake.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/wombat1.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/wombat2.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/excellent_alien.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/excellent_wood.jpg" alt="PIC"  
width="79" height="79" ></div>                                                                                                                                                                          <br />
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/snake_sushi.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/snake_corn.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/wombat3.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/wombat4.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/excellent_boat.jpg" alt="PIC"  
width="79" height="79" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/excellent_glass.jpg" alt="PIC"  
width="79" height="79" ></div>                                                                                                                                                                          <br />
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">D</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">A giant cobra snake made from X. X </span><span 
class="cmsy-6">&#x2208;</span><span 
class="ptmr8t-x-x-60">{</span><span 
class="ptmri8t-x-x-60">&#8220;salad&#8221;, &#8220;pancakes&#8221;,</span>
<span 
class="ptmri8t-x-x-60">&#8220;sushi&#8221;, &#8220;corn&#8221;</span><span 
class="ptmr8t-x-x-60">}</span></div>
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">E</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">A wombat sits in a yellow beach chair, while sipping a martini</span>
<span 
class="ptmri8t-x-x-60">that is on  his  laptop  keyboard.  The  wombat  is  wearing  a  white</span>
<span 
class="ptmri8t-x-x-60">panama hat and a floral Hawaiian shirt. Out-of-focus palm trees</span>
<span 
class="ptmri8t-x-x-60">in the background. DSLR photograph. Wide-angle view.</span></div>
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">F</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">The saying "BE EXCELLENT TO EACH OTHER" ...</span><span 
class="ptmr8t-x-x-60">, (a) brick</span>
<span 
class="ptmr8t-x-x-60">wall and alien (b) driftwood. (c) old wooden boat with reflection.</span>
<span 
class="ptmr8t-x-x-60">(d) stained glass. (See text for full prompts.)</span></div>                                                        <br />
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/porsche1977.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/porsche1997.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/porsche2017.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/rembrandt.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/vangogh.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/hokusai.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/pangolin_tennis.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/pangolin_soccer.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/pangolin_basketball.jpg" alt="PIC"  
width="52" height="52" ></div>                                                                                                                                                                          <br />
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/corvette1977.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/corvette1997.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/corvette2017.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/pixelart.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/pointilism.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/cubism.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/toucan_tennis.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/toucan_soccer.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/toucan_basketball.jpg" alt="PIC"  
width="52" height="52" ></div>                                                                                                                                                                          <br />
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/ford1977.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/ford1997.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/ford2017.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/egyptian.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/chinese.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/madhubani.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/panda_tennis.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/panda_soccer.jpg" alt="PIC"  
width="52" height="52" ></div>
<div class="multicolumn"  style="white-space:nowrap; text-align:center;"><img 
src="figures/cherries/panda_basketball.jpg" alt="PIC"  
width="52" height="52" ></div>                                                                                                                                                                          <br />
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">G</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">Three-quarters front view of a X Y Z coming around a curve</span>
<span 
class="ptmri8t-x-x-60">in a mountain road and looking over a green valley on a cloudy</span>
<span 
class="ptmri8t-x-x-60">day. DSLR photograph. X </span><span 
class="cmsy-6">&#x2208;</span><span 
class="ptmr8t-x-x-60">{</span><span 
class="ptmri8t-x-x-60">blue, red, yellow</span><span 
class="ptmr8t-x-x-60">}, </span><span 
class="ptmri8t-x-x-60">Y </span><span 
class="cmsy-6">&#x2208;</span><span 
class="ptmr8t-x-x-60">{</span><span 
class="ptmri8t-x-x-60">1977, 1997,</span>
<span 
class="ptmri8t-x-x-60">2017</span><span 
class="ptmr8t-x-x-60">}, </span><span 
class="ptmri8t-x-x-60">Z </span><span 
class="cmsy-6">&#x2208;</span><span 
class="ptmr8t-x-x-60">{</span><span 
class="ptmri8t-x-x-60">Porsche 911, Corvette, Ford F-150</span><span 
class="ptmr8t-x-x-60">}</span></div>
<div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">H</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">A raccoon  wearing  formal  clothes,  wearing  a  tophat  and</span>
<span 
class="ptmri8t-x-x-60">holding  a  cane.  The  raccoon  is  holding  a  garbage  bag.  Oil</span>
<span 
class="ptmri8t-x-x-60">painting  in  the  style  of  X.  X  </span><span 
class="cmsy-6">&#x2208; </span><span 
class="ptmr8t-x-x-60">{</span><span 
class="ptmri8t-x-x-60">&#8220;Rembrandt&#8221;,  &#8220;Vincent  Van</span>
<span 
class="ptmri8t-x-x-60">Gogh&#8221;, &#8220;Hokusai&#8221;, &#8220;pixel art&#8221;, &#8220;pointillism&#8221;, &#8220;abstract cubism&#8221;,</span>
<span 
class="ptmri8t-x-x-60">&#8220;Egyptian  tomb  heiroglyphics&#8221;,  &#8220;traditional  Chinese  painting&#8221;,</span>
<span 
class="ptmri8t-x-x-60">&#8220;Madhubani art&#8221;</span><span 
class="ptmr8t-x-x-60">}</span></div>                                     <div class="multicolumn"  style="white-space:normal; text-align:left;"><span 
class="ptmb8t-x-x-60">I</span><span 
class="ptmr8t-x-x-60">. </span><span 
class="ptmri8t-x-x-60">A photo of an Athenian vase with a painting of X playing Y in</span>
                                          <span 
class="ptmri8t-x-x-60">the style of Egyptian hieroglyphics. X  </span><span 
class="cmsy-6">&#x2208;</span><span 
class="ptmr8t-x-x-60">{</span><span 
class="ptmri8t-x-x-60">&#8220;pandas&#8221;, &#8220;toucans&#8221;,</span>
                                          <span 
class="ptmri8t-x-x-60">&#8220;pangolins&#8221;</span><span 
class="ptmr8t-x-x-60">}, </span><span 
class="ptmri8t-x-x-60">Y </span><span 
class="cmsy-6">&#x2208;</span><span 
class="ptmr8t-x-x-60">{</span><span 
class="ptmri8t-x-x-60">&#8220;tennis&#8221;, &#8220;soccer&#8221;, &#8220;basketball&#8221;</span><span 
class="ptmr8t-x-x-60">}</span></div>
<br /></dd></dl>
       <a 
 id="x1-4r2"></a>
       <a 
 id="x1-5"></a>
<br />  <div class="caption" 
><span class="id">Figure&#x00A0;2: </span><span  
class="content">Selected Parti images. See Section <span 
class="ptmb8t-">??</span> for discussion.</span></div><!--tex4ht:label?: x1-4r -->
                                                                                                                                                                      
                                                                                                                                                                      
       </div><hr class="endfigure">
       <h3 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-10001"></a>Introduction</h3>
       People are generally able to conjure rich and detailed scenes through descriptions expressed in written or
       spoken language. Supporting the ability to generate images based on such descriptions can potentially
       unlock creative applications in many areas of life, including the arts, design, and multimedia content
       creation. Recent research on text-to-image generation, <span 
class="ptmri8t-">e.g</span>., DALL-E&#x00A0;[<span 
class="ptmb8t-">? </span>] and CogView&#x00A0;[<span 
class="ptmb8t-">? </span>], has
       made significant progress in generating high-fidelity images and demonstrating generalization
       capabilities to unseen combinations of objects and concepts. Both treat the task as a form of language
       modeling, from textual descriptions into visual words, and use modern sequence-to-sequence
       architectures like Transformers&#x00A0;[<span 
class="ptmb8t-">? </span>] to learn the relationship between language inputs and visual
       outputs. A key component of these approaches is the conversion of each image into a sequence of
       discrete units through the use of an image tokenizer such as dVAE&#x00A0;[<span 
class="ptmb8t-">? </span>] or VQ-VAE&#x00A0;[<span 
class="ptmb8t-">? </span>]. Visual
       tokenization essentially unifies the view of text and images so that both can be treated simply as
       sequences of discrete tokens&#8212;and thus amenable to sequence-to-sequence models. To that end,
       DALL-E and CogView employed decoder-only language models, similar to GPT&#x00A0;[<span 
class="ptmb8t-">? </span>], to learn
       from a large collection of potentially noisy text-image pairs&#x00A0;[<span 
class="ptmb8t-">? ? </span>]. Make-A-Scene&#x00A0;[<span 
class="ptmb8t-">? </span>] further
       expands on this two-stage modeling approach to support both text and scene-guided image
       generation.
       A different line of research with considerable momentum involves diffusion-based text-to-image models,
       such as GLIDE&#x00A0;[<span 
class="ptmb8t-">? </span>] and concurrent works DALL-E 2&#x00A0;[<span 
class="ptmb8t-">? </span>] (<span 
class="ptmri8t-">a.k.a</span>., unCLIP) and Imagen&#x00A0;[<span 
class="ptmb8t-">? </span>].
       These models eschew the use of discrete image tokens in favor of diffusion models&#x00A0;[<span 
class="ptmb8t-">? ? </span>] to
       directly generate images. These models improve zero-shot Fréchet Inception Distance (FID)
       scores on MS-COCO&#x00A0;[<span 
class="ptmb8t-">? </span>] and produce images of markedly higher-quality and greater aesthetic
       appeal compared to previous work. Even so, autoregressive models for text-to-image generation
       remain appealing given extensive prior work on scaling large language models&#x00A0;[<span 
class="ptmb8t-">? ? ? ? </span>] and
       advances in discretizing other modalities&#8211;such as images and audio&#8211;so that inputs in those
       modalities can be treated as language-like tokens. This work presents the Pathways Autoregressive
       Text-to-Image (<span 
class="ptmri8t-">Parti</span>) model, which generates high-quality images from text descriptions, including
       photo-realistic ones, paintings, drawings, and more (see Fig.&#x00A0;<span 
class="ptmb8t-">??</span> &amp; <span 
class="ptmb8t-">??</span>). We show that with a
       ViT-VQGAN&#x00A0;[<span 
class="ptmb8t-">? </span>] image tokenizer, scaling autoregressive models is an effective way to improve
       text-to-image generation, enabling such models to accurately integrate and visually convey world
       knowledge.
       Parti is a sequence-to-sequence model based on the Transformer&#x00A0;[<span 
class="ptmb8t-">? </span>], an architecture critical to
       performance on many tasks, including machine translation&#x00A0;[<span 
class="ptmb8t-">? </span>], speech recognition&#x00A0;[<span 
class="ptmb8t-">? ? </span>],
       conversational modeling&#x00A0;[<span 
class="ptmb8t-">? </span>], image captioning&#x00A0;[<span 
class="ptmb8t-">? </span>], and many others. Parti takes text tokens as
       inputs to an encoder and autoregressively predicts discrete image tokens with a decoder (see
       Figure&#x00A0;<span 
class="ptmb8t-">??</span>). The image tokens are produced by the Transformer-based ViT-VQGAN image
       tokenizer&#x00A0;[<span 
class="ptmb8t-">? </span>], which produces higher-fidelity reconstructed outputs and has better codebook
       utilization compared with dVAE [<span 
class="ptmb8t-">? </span>], VQ-VAE [<span 
class="ptmb8t-">? </span>], and VQGAN&#x00A0;[<span 
class="ptmb8t-">? </span>]. Parti is conceptually
       simple: all of its components &#8211; encoder, decoder and image tokenizer &#8211; are based on standard
       Transformers&#x00A0;[<span 
class="ptmb8t-">? </span>]. This simplicity makes it straightforward to scale our models using standard
       techniques and existing infrastructure&#x00A0;[<span 
class="ptmb8t-">? ? ? ? </span>]. To explore the limits of this two-stage text-to-image
       framework, we scale the parameter size of Parti models up to 20B, and observe consistent quality
       improvements in terms of both text-image alignment and image quality. The 20B Parti model
       achieves new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on
       MS-COCO.
       While most recent work has focused exclusively on the MS-COCO benchmark, we also show that strong
       zero-shot and finetuned results can be achieved on the Localized Narratives dataset&#x00A0;[<span 
class="ptmb8t-">? </span>], which has
       descriptions that are four times longer than MS-COCO&#8217;s on average. These results demonstrate the strong
       generalization capability of Parti to longer descriptions, allowing us to pile on considerable
       complexity in our explorations with the model (see examples in Figure&#x00A0;<span 
class="ptmb8t-">??</span> and the Appendix,
       and the discussion of <span 
class="ptmri8t-">growing a cherry tree </span>in Section&#x00A0;<span 
class="ptmb8t-">??</span>). Nevertheless, existing captioning /
       descriptioning datasets are limited to photographs and descriptions of their contents, but much
       of the appeal of text-to-image models is that they can produce novel outputs for fantastical
                                                                                                                                                                      
                                                                                                                                                                      
       prompts. Given this, we introduce <span 
class="ptmri8t-">PartiPrompts </span>(P<span 
class="cmr-10">2</span>), a rich set of over 1600 (English) prompts
       curated to measure model capabilities across a variety of categories and controlled dimensions of
       difficulty.<span class="footnote-mark">¶</span><a 
 id="x1-1001f5"></a><span class="footnote-mark"><a 
 id="fn5x1"> ¶</a></span><span 
class="ptmr8t-x-x-90">Imagen [</span><span 
class="ptmb8t-x-x-90">? </span><span 
class="ptmr8t-x-x-90">] introduces 200 prompts in DrawBench for similar purposes. See Section</span> <span 
class="ptmb8t-x-x-90">??</span> <span 
class="ptmr8t-x-x-90">for discussion.</span>
       Each prompt in P<span 
class="cmr-10">2 </span>is associated with both a broad category (out of 12 categories, ranging from abstract to
       animals, vehicles, and world knowledge) and a challenge dimension (out of 11 aspects, ranging from basic,
       to quantity, words &amp; symbols, linguistics, and complex). Our detailed analyses and human evaluations on
       MS-COCO, Localized Narratives and P<span 
class="cmr-10">2</span>, along with extensive discussion of Parti&#8217;s limitations (Section <span 
class="ptmb8t-">??</span>)
       give a comprehensive picture of the strengths and weaknesses of Parti models&#8212;and establish autoregressive
       models as strong contenders for high-quality, broadly capable, open-domain text-to-image generation
       models.
       Our main contributions include: (1) We demonstrate that autoregressive models can achieve
       <span 
class="ptmri8t-">state-of-the-art </span>performance: 7.23 zero-shot and 3.22 finetuned FID on MS-COCO, and 15.97 zero-shot
       and 8.39 finetuned FID on Localized Narratives; (2) <span 
class="ptmri8t-">Scale matters</span>: our largest Parti model
       (20B) is most capable at high-fidelity photo-realistic image generation and supports content-rich
       synthesis, particularly those involving complex compositions and world knowledge; and (3) We also
       introduce a holistic benchmark, the <span 
class="ptmri8t-">PartiPrompts </span>(P<span 
class="cmr-10">2</span>), propose a novel concept of <span 
class="ptmri8t-">Growing</span>
       <span 
class="ptmri8t-">a Cherry Tree</span>, establish a new precedent regarding identifying limitations of text-to-image
       generation models, and provide a detailed breakdown, with exemplars, for error types we observed.
       <hr class="figure"><div class="figure" 
>
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
       <img 
src="figures/parti.jpg" alt="PIC"  
width="298" height="298" > <a 
 id="x1-1002r3"></a>
       <a 
 id="x1-1003"></a>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3: </span><span  
class="content">Overview of Parti sequence-to-sequence autoregressive model (left) for text-to-image
generation with ViT-VQGAN as the image tokenizer&#x00A0;[<span 
class="ptmb8t-">? </span>] (right).</span></div><!--tex4ht:label?: x1-1002r1 -->
                                                                                                                                                                      
                                                                                                                                                                      
       </div><hr class="endfigure">
       <h3 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-20002"></a>Parti Model</h3>
       Similar to DALL-E&#x00A0;[<span 
class="ptmb8t-">? </span>], CogView&#x00A0;[<span 
class="ptmb8t-">? </span>], and Make-A-Scene&#x00A0;[<span 
class="ptmb8t-">? </span>], Parti is a two-stage model,
       composed of an image tokenizer and an autoregressive model, as highlighted in Figure&#x00A0;<span 
class="ptmb8t-">??</span>. The
       first stage involves training a tokenizer that turns an image into a sequence of discrete visual
       tokens for training and reconstructs an image at inference time. The second stage trains an
       autoregressive sequence-to-sequence model that generates image tokens from text tokens. We describe
       details for these two stages below, together with other techniques for building high-performing
       autoregressive text-to-image models, such as text encoder pretraining, classifier-free guidance, and
       reranking.
       <h4 class="subsectionHead"><span class="titlemark">2.1    </span> <a 
 id="x1-30002.1"></a>Image Tokenizer</h4>
       Autoregressive text-to-image models must linearize 2D images into 1D sequences of patch representations.
       In the limit, these are just pixels, as with iGPT&#x00A0;[<span 
class="ptmb8t-">? </span>], but this requires modeling very long sequences even for
       small images (<span 
class="ptmri8t-">e.g</span>., a <img 
src="main0x.png" alt="256&#x00D7;256 &#x00D7; 3 "  class="math" > RGB image leads to 196,608 rasterized values). Worse, it is based on a
       very low-level representation of the inputs rather than a richer one informed by the position of a pixel in
       the context of the image. Previous work&#x00A0;[<span 
class="ptmb8t-">? ? ? ? </span>] addressed this problem by using a discrete
       variational auto-encoder to learn quantized representations of image patches over a collection of raw
       images. Instead of learning representations that can take any value in the latent space, a visual
       codebook is learned that maps a patch embedding to its nearest codebook entry, which is a
       learned and indexable location in the latent space. These entries can be thought of as visual word
       <span 
class="ptmri8t-">types</span>, and the appearance of any of these words in a patch in a given image is thus an image
       <span 
class="ptmri8t-">token</span>.
       To be most useful for the second stage model, the image tokenizer needs to learn an effective visual
       codebook that supports balanced usage of its entries across a broad range of images. It also must support
       reconstruction of a sequence of visual tokens as a high-quality output image. We use ViT-VQGAN [<span 
class="ptmb8t-">? </span>]
       with techniques including <img 
src="main1x.png" alt="&#x2113;2  "  class="math" >-normalization codes and factorized codes, which contribute to
       training stability, reconstruction quality and codebook usage. A ViT-VQGAN image tokenizer
       is trained with the same losses and hyper-parameters as&#x00A0;[<span 
class="ptmb8t-">? </span>] on images of our training data
       (see Section&#x00A0;<span 
class="ptmb8t-">??</span>). We first train a ViT-VQGAN-Small configuration (8 blocks, 8 heads, model
       dimension 512, and hidden dimension 2048 as shown in Table 2 of&#x00A0;[<span 
class="ptmb8t-">? </span>], with about 30M total
       parameters), and learn 8192 image token classes for the codebook. We note that the second stage
       autoregressive encoder-decoder <span 
class="ptmri8t-">training </span>only relies on the encoder and the codebook of a learned
       image tokenizer. To further improve visual acuity of the reconstructed images after second-stage
       encoder-decoder training, we freeze the tokenizer&#8217;s encoder and codebook, and finetune a larger-size
       tokenizer decoder (32 blocks, 16 heads, model dimension 1280, and hidden dimension 5120, with
       about 600M total parameters). We use <img 
src="main2x.png" alt="256&#x00D7;256 "  class="math" > resolution for the image tokenizer&#8217;s input and
       output.
       We notice visual pixelation patterns in some of the output images of ViT-VQGAN when zooming in
       (see Appendix&#x00A0;<span 
class="ptmb8t-">??</span>), and further find ill-conditioned weight matrices of the output projection
       layer before the sigmoid activation function. As a fix, we remove the final sigmoid activation
       layer and the logit-laplace loss, exposing the raw values as RGB pixel values (in range [0, 1]).
       Conveniently, this fix can be hot-swappable into an already trained image tokenizer by finetuning the
       decoder.
       Finally, while images of resolution <img 
src="main3x.png" alt="256&#x00D7;256 "  class="math" > capture most of the contents, structures and textures,
       higher-resolution images have greater visual impact. To this end, we employ a simple super-resolution
       module on top of the image tokenizer, shown in Figure&#x00A0;<span 
class="ptmb8t-">??</span>. Stacked convolutional layers with residual
       connections are used as the super-resolution network module following WDSR&#x00A0;[<span 
class="ptmb8t-">? </span>] (12 residual blocks
       with 128 channels). It is learned with the same losses of ViT-VQGAN (perceptual loss, StyleGAN
                                                                                                                                                                      
                                                                                                                                                                      
       loss and <img 
src="main4x.png" alt="&#x2113;2  "  class="math" > loss with same loss weighting in&#x00A0;[<span 
class="ptmb8t-">? </span>]), mapping from reconstructed images to
       higher-resolution reconstructed images. The super-resolution module has about 15M parameters for
       the <img 
src="main5x.png" alt="512&#x00D7;512 "  class="math" > version and about 30M parameters for the <img 
src="main6x.png" alt="1024&#x00D7;1024 "  class="math" > version. We note that
       diffusion models could also be used here as iterative refinement super-resolution modules, as also
       demonstrated in DALL-E 2&#x00A0;[<span 
class="ptmb8t-">? </span>] and Imagen&#x00A0;[<span 
class="ptmb8t-">? </span>], either with or without conditioning on text
       inputs.
       <hr class="figure"><div class="figure" 
>
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
        <img 
src="figures/sr_vit_vqgan-.png" alt="PIC"  
width="397" height="397" > <a 
 id="x1-3001r4"></a>
       <a 
 id="x1-3002"></a>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;4: </span><span  
class="content">A learned super-resolution module to upsample <img 
src="main7x.png" alt="256&#x00D7; 256 "  class="math" > images to higher-resolution
<img 
src="main8x.png" alt="1024&#x00D7; 1024 "  class="math" > ones based on a frozen ViT-VQGAN image tokenizer. The super-resolution module
takes <img 
src="main9x.png" alt="256 &#x00D7; 256 "  class="math" > images as inputs without conditioning on text inputs.</span></div><!--tex4ht:label?: x1-3001r2 -->
                                                                                                                                                                      
                                                                                                                                                                      
       </div><hr class="endfigure">
       <h4 class="subsectionHead"><span class="titlemark">2.2    </span> <a 
 id="x1-40002.2"></a>Encoder-Decoder for Text-to-Image Generation</h4>
       <div class="table">
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
       <a 
 id="x1-4001r1"></a>
       <a 
 id="x1-4002"></a>
<br /> <div class="caption" 
><span class="id">Table&#x00A0;1: </span><span  
class="content"> Size variants of Parti. Both encoder and decoder are based on Transformers&#x00A0;[<span 
class="ptmb8t-">? </span>]. The
self-attention layer in decoder transformer is causally masked. Parameters of ViT-VQGAN image
tokenization are not included in the total parameter count and can be found in Section&#x00A0;<span 
class="ptmb8t-">??</span>.</span></div><!--tex4ht:label?: x1-4001r2 -->
                                                                                                                                                                      
                                                                                                                                                                      
       </div>
       As shown in Figure&#x00A0;<span 
class="ptmb8t-">??</span>, a standard encoder-decoder Transformer model is trained at the second stage, by
       treating text-to-image as a sequence-to-sequence modeling problem. The model takes text as input and is
       trained using next-token prediction of rasterized image latent codes generated from the first stage
       image tokenizer. For text encoding, we build a sentence-piece model&#x00A0;[<span 
class="ptmb8t-">? ? </span>] of vocabulary size
       16,000 on a sampled text corpus from the training data (Section&#x00A0;<span 
class="ptmb8t-">??</span>). Image tokens are produced
       by a learned ViT-VQGAN image tokenizer (see Section&#x00A0;<span 
class="ptmb8t-">??</span>). At inference time, the model
       samples image tokens autoregressively, which are later decoded into pixels using the ViT-VQGAN
       decoder.
       We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024 (<span 
class="ptmri8t-">i.e</span>.,
       <img 
src="main10x.png" alt="32&#x00D7;32 "  class="math" > latent codes from a <img 
src="main11x.png" alt="256 &#x00D7; 256 "  class="math" > input image). As an example, the 67-word description of the
       Starry Night prompt given in Figure&#x00A0;<span 
class="ptmb8t-">??</span> has a total length of 92 text tokens. All models use conv-shaped
       masked sparse attention&#x00A0;[<span 
class="ptmb8t-">? </span>]. We train four size variants ranging from 350 million to 20 billion
       parameters, as detailed in Table&#x00A0;<span 
class="ptmb8t-">??</span>. Specifically, we configure the Transformers following
       previous practice of those in scaling language models with default expansion ratio of <img 
src="main12x.png" alt="4&#x00D7; "  class="math" > in
       MLP dimensions. We double the number of heads when the model dimension is doubled. In the
       current scaling variants, our configuration prefers a larger decoder for modeling image tokens
       and as a result the decoder has more layers (<span 
class="ptmri8t-">e.g</span>., <img 
src="main13x.png" alt="3&#x00D7; "  class="math" > in the 3B model and <img 
src="main14x.png" alt="4&#x00D7; "  class="math" > in the 20B
       model).
       Most of the existing two-stage text-to-image generation models, including DALL-E&#x00A0;[<span 
class="ptmb8t-">? </span>], CogView&#x00A0;[<span 
class="ptmb8t-">? </span>] and
       Make-A-Scene&#x00A0;[<span 
class="ptmb8t-">? </span>], are decoder-only models. We found that at the model scale of 350-million to
       750-million parameters, the encoder-decoder variants of Parti outperformed decoder-only ones, both in
       terms of training loss and text-to-image generation quality in our early exploration. We thus chose to focus
       on scaling the encoder-decoder models.
       <h4 class="subsectionHead"><span class="titlemark">2.3    </span> <a 
 id="x1-50002.3"></a>Text Encoder Pretraining</h4>
       The encoder-decoder architecture also decouples text encoding from image-token generation, so it is
       straightforward to explore warm-starting the model with a pretrained text encoder. Intuitively, a text
       encoder with representations based on generic language training should be more capable at
       handling visually-grounded prompts. We pretrain the text encoder on two datasets: the Colossal
       Clean Crawled Corpus (C4)&#x00A0;[<span 
class="ptmb8t-">? </span>] with BERT&#x00A0;[<span 
class="ptmb8t-">? </span>] pretraining objective, and our image-text
       data (see Section <span 
class="ptmb8t-">??</span>) with a contrastive learning objective (image encoder from the contrastive
       pretraining is not used). After pretraining, we continue training both encoder and decoder for
       text-to-image generation with softmax cross-entropy loss on a vocabulary of 8192 discrete image
       tokens.
       The text encoder after pretraining performs comparably to BERT [<span 
class="ptmb8t-">? </span>] on GLUE (see Appendix&#x00A0;<span 
class="ptmb8t-">??</span>,
       Table&#x00A0;<span 
class="ptmb8t-">??</span>); however, the text encoder degrades after the full encoder-decoder training process on
       text-to-image generation. We leave this observation as a future research topic on the difference and
       unification of generic language representation and visually-grounded language representation. Still, the
       text-encoder pretraining <span 
class="ptmri8t-">marginally </span>helps text-to-image generation loss with 3B-parameter Parti models, so
       pretraining is used by default in our 20B model. We provide detailed training loss, GLUE evaluation of text
       encoders, and some qualitative comparison in Appendix&#x00A0;<span 
class="ptmb8t-">??</span>.
       <h4 class="subsectionHead"><span class="titlemark">2.4    </span> <a 
 id="x1-60002.4"></a>Classifier-Free Guidance and Reranking</h4>
       Classifier-free guidance&#x00A0;[<span 
class="ptmb8t-">? </span>] (CF-guidance in short) is critical in the context of improving the sample
       quality of diffusion models&#x00A0;[<span 
class="ptmb8t-">? ? ? </span>] without pretrained classifiers. In this setup, a generative model <span 
class="cmmi-10">G</span>
       is trained to be able to perform unconditional generation <span 
class="cmmi-10">G</span><span 
class="cmr-10">(</span><span 
class="cmbx-10">z</span><span 
class="cmr-10">) </span>(where <span 
class="cmbx-10">z </span>represents random
       noise) and conditional generation <span 
class="cmmi-10">G</span><span 
class="cmr-10">(</span><span 
class="cmbx-10">z</span><span 
class="cmmi-10">,</span><span 
class="cmbx-10">c</span><span 
class="cmr-10">) </span>(where <span 
class="cmbx-10">c </span>represents some condition, such as language
       descriptions). It is implemented as randomly dropping out the conditional vector (masking out
       or switching to a learned embedding) with some probability. During the inference process,
                                                                                                                                                                      
                                                                                                                                                                      
       sampling of an output <span 
class="cmmi-10">I </span>is done by using a linear combination of the unconditional and conditional
       predictions:
       <table 
class="equation"><tr><td><a 
 id="x1-6001r1"></a>
       <center class="math-display" >
       <img 
src="main15x.png" alt="I = G(z)+ &#x03BB;(G(z,c)- G(z)),
       " class="math-display" ></center></td><td class="equation-label">(1)</td></tr></table>
       where <span 
class="cmmi-10">&#x03BB; </span>is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it
       decreases the unconditional likelihood of the sample while increasing the conditional likelihood,
       which can be viewed as encouraging alignment between the generated sample and the text
       condition.
       Classifier-free guidance has been similarly applied in the context of autoregressive models for text-to-image
       generation&#x00A0;[<span 
class="ptmb8t-">? ? </span>] to great effect. Make-A-Scene&#x00A0;[<span 
class="ptmb8t-">? </span>] finetunes the model by randomly replacing the text
       prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits
       sampled from an unconditional model and a conditional model on a text prompt. We also apply
       CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment,
       especially on challenging text prompts.
       With batch-sampled images per text prompt, contrastive reranking is used in DALL-E&#x00A0;[<span 
class="ptmb8t-">? </span>] which produces
       image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is
       complementary to classifier-free guidance. Compared with the 512 images used in DALL-E&#x00A0;[<span 
class="ptmb8t-">? </span>], we
       sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set
       based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa)&#x00A0;[<span 
class="ptmb8t-">? </span>].
       A CoCa base-size model (Table 1 in&#x00A0;[<span 
class="ptmb8t-">? </span>]) is trained on the same dataset with details in Section&#x00A0;<span 
class="ptmb8t-">??</span>. We note
       that reranking over a small set of batch-sampled images is computationally cheap in the text-to-image
       sampling process, and produces helpful image-text alignment scores among diverse image outputs.
       <hr class="figure"><div class="figure" 
>
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
        <img 
src="figures/shard3b-.png" alt="PIC"  
width="238" height="238" > <a 
 id="x1-6002r5"></a>
       <a 
 id="x1-6003"></a>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;5: </span><span  
class="content">4-way in-layer model parallelism with fully partitioned activations used to scale the 3B
model training. The figure shows a simplified Transformer feed-forward layer (with the sequence
dimension omitted); each color represents data on one device. We also use 128-way data parallelism.</span></div><!--tex4ht:label?: x1-6002r2 -->
                                                                                                                                                                      
                                                                                                                                                                      
       </div><hr class="endfigure">
       <h3 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-70003"></a>Scaling</h3>
       We implement our models in Lingvo&#x00A0;[<span 
class="ptmb8t-">? </span>] and scale with GSPMD&#x00A0;[<span 
class="ptmb8t-">? </span>] on CloudTPUv4 hardware for both
       training and inference. GSPMD is an XLA compiler-based model partitioning system that allows
       us to treat a cluster of TPUs as a single virtual device and use <span 
class="ptmri8t-">sharding annotations </span>on a few
       tensors to instruct the compiler to automatically distribute data and compute on thousands of
       devices.
       <span 
class="ptmb8t-">Training. </span>We train both 350M and 750M models simply with data parallelism. For the 3B model, we use
       4-way in-layer model parallelism (see Figure&#x00A0;<span 
class="ptmb8t-">??</span>), and 128-way data parallelism. Partitioning a single
       dimension in each tensor is sufficient to scale a 3B model. The model weights are partitioned on the
       feed-forward hidden dimension and the number of attention heads dimension; the internal activation tensors
       of the feed-forward and attention layers are also partitioned on the hidden and heads dimensions. One
       difference from Megatron-LM&#x00A0;[<span 
class="ptmb8t-">? </span>] is we fully partition the output activations of feed-forward
       and attention layers on a different dimension, with the details illustrated as the <span 
class="ptmri8t-">finalized 2d</span>
       <span 
class="ptmri8t-">sharding </span>in the GSPMD work&#x00A0;[<span 
class="ptmb8t-">? </span>]. This strategy will result in <span 
class="ectt-1000">ReduceScatter </span>and <span 
class="ectt-1000">AllGather</span>
       communication patterns instead of <span 
class="ectt-1000">AllReduce</span>, which significantly reduce peak activation
       memory.
       <hr class="figure"><div class="figure" 
>
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
        <img 
src="figures/pipeline-.png" alt="PIC"  
width="317" height="318" > <a 
 id="x1-7001r6"></a>
       <a 
 id="x1-7002"></a>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;6:  </span><span  
class="content">An  illustration  of  16-stage  GSPMD  pipelines  used  to  scale  the  20B  model  training.
The  figure  shows  how  the  16  devices  are  used  for  data  parallelism  in  the  quantizer,  embedding
and softmax layers, but repurposed for pipelining in the encoder and decoder layers. Each color
represents data or layer assigned to one device. The decoder uses 4-round circular schedule to further
reduce the pipeline bubble ratio. On top of this, we use additional 64-way data parallelism for all
layers.</span></div><!--tex4ht:label?: x1-7001r3 -->
                                                                                                                                                                      
                                                                                                                                                                      
       </div><hr class="endfigure">
       The 20B model has 16 encoder layers, and 64 decoder layers (see Table&#x00A0;<span 
class="ptmb8t-">??</span>). The size of the weights per
       layer is moderate (as opposed to being very wide), which makes pipeline parallelism&#x00A0;[<span 
class="ptmb8t-">? </span>] a good option for
       scaling. We use a generic pipelining wrapper layer allowing us to specify a single-stage program, which
       will later be automatically transformed into a multi-stage pipelining program; the wrapper layer uses
       vectorization and shifting buffers to reduce pipelining into a tensor partitioning problem (see Section
       3.3 of&#x00A0;[<span 
class="ptmb8t-">? </span>]). Thus, all lower-level infrastructure can be reused for pipelining. There are two
       additional benefits in adopting GSPMD pipelining: 1) it allows us to conveniently configure
       pipelines within model sub-components, simplifying the overall complexity for encoder-decoder
       models, and 2) since pipelining is implemented as tensor partitioning on vectorized programs,
       we can reuse the same set of devices for other types of parallelism outside the transformer
       layers.
       We configure the model to have separate encoder and decoder pipelines, each with 16 stages. We also use
       64-way data parallelism in addition to pipelining. However this makes per-core batch size small, exposing
       an additional challenge of excessive pipeline stalls due to inter-stage data dependency (known as <span 
class="ptmri8t-">bubbles</span>
       in pipeline parallelism&#x00A0;[<span 
class="ptmb8t-">? </span>]). To reduce the ratio of bubbles, we adapt the <span 
class="ptmri8t-">circular schedule</span>
       as described in [<span 
class="ptmb8t-">? </span>] in the decoder pipeline (a similar technique was also proposed in [<span 
class="ptmb8t-">? </span>]),
       where the 4 layers in each stage are executed in a round-robin order. Outside the encoder and
       decoder, we use the same set of devices to do data parallelism instead of pipelining for the
       embedding, softmax, and image tokenizer layers. Figure&#x00A0;<span 
class="ptmb8t-">??</span> illustrates the overall distributed training
       strategy.
       During training, Adafactor&#x00A0;[<span 
class="ptmb8t-">? </span>] optimizer is used to save memory with <span 
class="cmmi-10">&#x03B2;</span><sub><span 
class="cmr-7">1</span></sub> <span 
class="cmr-10">= 0</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">9</span>, <span 
class="cmmi-10">&#x03B2;</span><sub><span 
class="cmr-7">2</span></sub> <span 
class="cmr-10">= 0</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">96 </span>and decoupled
       weight decay value of <span 
class="cmr-10">4</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">5 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">10</span><sup><span 
class="cmsy-7">-</span><span 
class="cmr-7">2</span></sup>. The first moments of optimizer slot variables are also quantized from
       float32 to int8. We use default dropout ratio 0.1 for all models in both encoder and decoder. A deterministic
       version of dropout layer as well as a vectorized version of Adafactor optimizer are used in the 20B model to
       enable training pipelined models. Data types are cast to bfloat16 for attention projection and
       feed-forward transformers layers, while all layer norms and model output are kept as float32. We use
       a default learning rate of 4.5e-5 and exponential learning rate schedule with 5,000 warm-up
       steps. Exponential decaying starts at training steps 85,000 with a total of 450,000 steps and final
       ratio of 0.025. We use a global batch size of 8192 during training. We do not use exponential
       moving average of the model weights to save device memory. Conv-shaped sparse attention is
       used in the decoder transformers, similar to DALL-E&#x00A0;[<span 
class="ptmb8t-">? </span>] (Appendix B.1. Architecture, Fig.
       11). We additionally clip gradient norm to a value of 4.0 to stabilize the training, especially at
       the beginning. At the output of both the encoder and decoder, we apply an additional layer
       normalization.
       <span 
class="ptmb8t-">Inference. </span>Our primary goal for inference optimization is to speed up small-batch image generation. We
       choose in-layer model parallelism for both the 3B and 20B models. As opposed to training, we do not fully
       partition the output activations for feed-forward and attention layers for inference; this is because 1) each
       step of the autoregressive decoding produces much smaller tensors and (at the time of writing) <span 
class="ectt-1000">AllReduce</span>
       performs better on small data, 2) activation memory is not a concern during inference, which does not have
       a backward pass.
       <h3 class="sectionHead"><span class="titlemark">4    </span> <a 
 id="x1-80004"></a>Training and Evaluation Datasets</h3>
       <h4 class="subsectionHead"><span class="titlemark">4.1    </span> <a 
 id="x1-90004.1"></a>Training Datasets</h4>
       We train on a combination of image-text datasets for all Parti models. The data includes the publicly
       available LAION-400M dataset&#x00A0;[<span 
class="ptmb8t-">? </span>]; FIT400M, a filtered subset of the full 1.8 billion examples used to
       train the ALIGN model&#x00A0;[<span 
class="ptmb8t-">? </span>]; JFT-4B dataset&#x00A0;[<span 
class="ptmb8t-">? </span>], which has images with text annotation labels. For textual
       descriptions of JFT, we randomly switch between the original labels as text (concatenated if an
       image has multiple labels) or machine-generated captions from a SimVLM model&#x00A0;[<span 
class="ptmb8t-">? </span>]. We
                                                                                                                                                                      
                                                                                                                                                                      
       discuss the limitations of the data in Section&#x00A0;<span 
class="ptmb8t-">??</span>. For all image inputs, we follow the DALL-E
       dVAE input processing (Section A.2. Training in&#x00A0;[<span 
class="ptmb8t-">? </span>]) for image tokenizer training and the
       DALL-E Transformer input processing (Section B.2. Training in&#x00A0;[<span 
class="ptmb8t-">? </span>]) for encoder-decoder
       training.
       <h4 class="subsectionHead"><span class="titlemark">4.2    </span> <a 
 id="x1-100004.2"></a>Evaluation Datasets</h4>
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      
       We evaluate our models on MS-COCO (2014) [<span 
class="ptmb8t-">? </span>] and Localized Narratives [<span 
class="ptmb8t-">? </span>], summarized in Table&#x00A0;<span 
class="ptmb8t-">??</span>.
       MS-COCO is the current standard dataset for measuring both zero-shot and finetuned text-to-image
       generation performance, which makes it a consistent point of comparison with prior work.
       However, MS-COCO captions are short, high-level characterizations of their corresponding images.
       For a more comprehensive evaluation, we also use the COCO portion of Localized Narratives
       (LN-COCO), which provides longer, detailed descriptions of images corresponding to the MS-COCO
       (2017) dataset, and compare Parti&#8217;s performance on LN-COCO against [<span 
class="ptmb8t-">? ? </span>]. These long-form
       descriptions are typically quite different from the descriptions used to train large text-to-image
       generation models. This provides a measure of generalization to out-of-domain distributions,
       as well as the finetuning capability of these models. Regardless of the community&#8217;s current
       focus on zero-shot performance, the ability to finetune effectively is also important for adapting
       an open-domain text-to-image generation model to work with a specific application area or
       domain.
       <h4 class="subsectionHead"><span class="titlemark">4.3    </span> <a 
 id="x1-110004.3"></a>PartiPrompts</h4>
                                                                                                                                                                      
                                                                                                                                                                      
                                                                                                                                                                      


