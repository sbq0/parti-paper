\begin{thebibliography}{100}

\bibitem{PathwaysProgram}
Jeff Dean.
\newblock Introducing pathways: A next-generation ai architecture, 2021.

\bibitem{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In {\em International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem{ding2021cogview}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da~Yin, Junyang
  Lin, Xu~Zou, Zhou Shao, Hongxia Yang, et~al.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{rolfe2016discrete}
Jason~Tyler Rolfe.
\newblock Discrete variational autoencoders.
\newblock {\em arXiv preprint arXiv:1609.02200}, 2016.

\bibitem{van2017neural}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem{changpinyo2021cc12m}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock {Conceptual 12M}: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em CVPR}, 2021.

\bibitem{align-paper}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 4904--4916. PMLR, 18--24 Jul 2021.

\bibitem{gafni2022make}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv
  Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock {\em arXiv preprint arXiv:2203.13131}, 2022.

\bibitem{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
  Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock {\em arXiv preprint arXiv:2112.10741}, 2021.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S.~Sara Mahdavi,
  Rapha~Gontijo Lopes, Tim Salimans, Jonathan Ho, David~J Fleet, and Mohammad
  Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em arXiv preprint arXiv:2205.11487}, 2022.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo
  Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
  Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao,
  Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,
  Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel
  Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben
  Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen
  Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi
  Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron
  Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,
  Marian Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{du2021glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
  Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock {\em arXiv preprint arXiv:2112.06905}, 2021.

\bibitem{palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{yu2021vector}
Jiahui Yu, Xin Li, Jing~Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander
  Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu.
\newblock Vector-quantized image modeling with improved vqgan.
\newblock {\em arXiv preprint arXiv:2110.04627}, 2021.

\bibitem{zhang2020transformer}
Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo,
  and Shankar Kumar.
\newblock Transformer transducer: A streamable speech recognition model with
  transformer encoders and rnn-t loss.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 7829--7833. IEEE, 2020.

\bibitem{gulati2020conformer}
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu~Zhang, Jiahui Yu,
  Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et~al.
\newblock Conformer: Convolution-augmented transformer for speech recognition.
\newblock {\em arXiv preprint arXiv:2005.08100}, 2020.

\bibitem{meena}
Daniel Adiwardana, Minh-Thang Luong, David~R. So, Jamie Hall, Noah Fiedel,
  Romal Thoppilan, Zi~Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and
  Quoc~V. Le.
\newblock Towards a human-like open-domain chatbot.
\newblock {\em arXiv preprint arXiv:2001.09977}, 2020.

\bibitem{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
  Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock {\em arXiv preprint arXiv:2205.01917}, 2022.

\bibitem{Esser21vqgan}
Patrick Esser, Robin Rombach, and Bj{\"{o}}rn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In {\em {CVPR}}, 2021.

\bibitem{megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism, 2019.

\bibitem{xu2021gspmd}
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul
  Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et~al.
\newblock Gspmd: general and scalable parallelization for ml computation
  graphs.
\newblock {\em arXiv preprint arXiv:2105.04663}, 2021.

\bibitem{PontTuset_eccv2020}
Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and
  Vittorio Ferrari.
\newblock Connecting vision and language with localized narratives.
\newblock In {\em ECCV}, 2020.

\bibitem{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em International Conference on Machine Learning}, pages
  1691--1703. PMLR, 2020.

\bibitem{yu2018wide}
Jiahui Yu, Yuchen Fan, Jianchao Yang, Ning Xu, Zhaowen Wang, Xinchao Wang, and
  Thomas Huang.
\newblock Wide activation for efficient and accurate image super-resolution.
\newblock {\em arXiv preprint arXiv:1808.08718}, 2018.

\bibitem{sennrich2015neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock {\em arXiv preprint arXiv:1508.07909}, 2015.

\bibitem{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock {\em arXiv preprint arXiv:1808.06226}, 2018.

\bibitem{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683}, 2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{ho2021classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock In {\em NeurIPS 2021 Workshop on Deep Generative Models and
  Downstream Applications}, 2021.

\bibitem{crowson2021classifier}
Katherine Crowson.
\newblock Classifier free guidance for autoregressive transformers.
\newblock 2021.

\bibitem{shen2019lingvo}
Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, Mia~X Chen, Ye~Jia,
  Anjuli Kannan, Tara Sainath, Yuan Cao, Chung-Cheng Chiu, et~al.
\newblock Lingvo: a modular and scalable framework for sequence-to-sequence
  modeling.
\newblock {\em arXiv preprint arXiv:1902.08295}, 2019.

\bibitem{gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia~Xu Chen, Dehao
  Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, Yonghui Wu, and Zhifeng Chen.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism, 2018.

\bibitem{megatron21}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay~Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,
  Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.
\newblock Efficient large-scale language model training on gpu clusters using
  megatron-lm, 2021.

\bibitem{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In {\em International Conference on Machine Learning}, pages
  4596--4604. PMLR, 2018.

\bibitem{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{zhai2021scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers, 2021.

\bibitem{wang2021simvlm}
Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock {\em arXiv preprint arXiv:2108.10904}, 2021.

\bibitem{trecs2020}
Jing~Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang.
\newblock Text-to-image generation grounded by fine-grained user attention.
\newblock {\em {WACV}}, 2021.

\bibitem{zhang2021cross}
Han Zhang, Jing~Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang.
\newblock Cross-modal contrastive learning for text-to-image generation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 833--842, 2021.

\bibitem{park2021benchmark}
Dong~Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach.
\newblock Benchmark for compositional text-to-image synthesis.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021.

\bibitem{vqdiffusion}
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo~Zhang, Dongdong Chen, Lu~Yuan,
  and Baining Guo.
\newblock Vector quantized diffusion model for text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2111.14822}, 2021.

\bibitem{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  4904--4916. PMLR, 2021.

\bibitem{avq_2020}
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and
  Sanjiv Kumar.
\newblock Accelerating large-scale inference with anisotropic vector
  quantization.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{Xu18}
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and
  Xiaodong He.
\newblock {AttnGAN}: Fine-grained text to image generation with attentional
  generative adversarial networks.
\newblock In {\em {CVPR}}, 2018.

\bibitem{Cho2022DallEval}
Jaemin Cho, Abhay Zala, and Mohit Bansal.
\newblock Dall-eval: Probing the reasoning skills and social biases of
  text-to-image generative transformers.
\newblock 2022.

\bibitem{cho2021vlt5}
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal.
\newblock Unifying vision-and-language tasks via text generation.
\newblock In {\em ICML}, 2021.

\bibitem{Papineni2002}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei{-}Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em ACL}, 2002.

\bibitem{Vedantam2015}
Ramakrishna Vedantam, C.~Lawrence Zitnick, and Devi Parikh.
\newblock Cider: Consensus-based image description evaluation.
\newblock In {\em CVPR}, 2015.

\bibitem{denkowski2014}
Michael Denkowski and Alon Lavie.
\newblock Meteor universal: Language specific translation evaluation for any
  target language.
\newblock In {\em Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, pages 376--380, Baltimore, Maryland, USA, June 2014.
  Association for Computational Linguistics.

\bibitem{spice2016}
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould.
\newblock {SPICE: Semantic Propositional Image Caption Evaluation}.
\newblock In {\em ECCV}, 2016.

\bibitem{cogview2}
Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.
\newblock Cogview2: Faster and better text-to-image generation via hierarchical
  transformers, 2022.

\bibitem{mindalle}
Chiheon Kim Doyup~Lee Saehoon~Kim, Sanghun~Cho and Woonhyuk Baek.
\newblock mindall-e on conceptual captions.
\newblock \url{https://github.com/kakaobrain/minDALL-E}, 2021.

\bibitem{Cho2020XLXMERTPC}
Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, and Aniruddha
  Kembhavi.
\newblock X-lxmert: Paint, caption and answer questions with multi-modal
  transformers.
\newblock {\em ArXiv}, abs/2009.11278, 2020.

\bibitem{Karpathy2017DeepVA}
Andrej Karpathy and Li~Fei-Fei.
\newblock Deep visual-semantic alignments for generating image descriptions.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  39:664--676, 2017.

\bibitem{erk-herbelot-2021-marry}
Katrin Erk and Aur{\'e}lie Herbelot.
\newblock How to marry a star: Probabilistic constraints for meaning in
  context.
\newblock In {\em Proceedings of the Society for Computation in Linguistics
  2021}, pages 451--453, Online, February 2021. Association for Computational
  Linguistics.

\bibitem{coyne2001wordseye}
Bob Coyne and Richard Sproat.
\newblock Wordseye: an automatic text-to-scene conversion system.
\newblock In {\em Proceedings of the 28th Annual Conference on Computer
  Graphics and Interactive Techniques}, 2001.

\bibitem{reed2016generative}
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and
  Honglak Lee.
\newblock Generative adversarial text to image synthesis.
\newblock In {\em International conference on machine learning}, pages
  1060--1069. PMLR, 2016.

\bibitem{Han17}
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang,
  and Dimitris Metaxas.
\newblock {StackGAN}: Text to photo-realistic image synthesis with stacked
  generative adversarial networks.
\newblock In {\em {ICCV}}, 2017.

\bibitem{Han17stackgan2}
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang,
  and Dimitris~N. Metaxas.
\newblock {StackGAN++}: Realistic image synthesis with stacked generative
  adversarial networks.
\newblock {\em TPAMI}, 2018.

\bibitem{reed2016learning}
Scott~E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and
  Honglak Lee.
\newblock Learning what and where to draw.
\newblock {\em {NeurIPS}}, 29, 2016.

\bibitem{HongYCL18}
Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee.
\newblock Inferring semantic layout for hierarchical text-to-image synthesis.
\newblock In {\em {CVPR}}, 2018.

\bibitem{HinzHW19}
Tobias Hinz, Stefan Heinrich, and Stefan Wermter.
\newblock Generating multiple objects at spatially distinct locations.
\newblock In {\em {ICLR}}, 2019.

\bibitem{dallemini}
Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc
  Le~Khac, Luke Melas, and Ritobrata Ghosh.
\newblock Dall·e mini, 7 2021.

\bibitem{chang2022maskgit}
Huiwen Chang, Han Zhang, Lu~Jiang, Ce~Liu, and William~T Freeman.
\newblock Maskgit: Masked generative image transformer.
\newblock {\em arXiv preprint arXiv:2202.04200}, 2022.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{ho2022cascaded}
Jonathan Ho, Chitwan Saharia, William Chan, David~J Fleet, Mohammad Norouzi,
  and Tim Salimans.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock {\em Journal of Machine Learning Research}, 23(47):1--33, 2022.

\bibitem{karras2020analyzing}
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
  Timo Aila.
\newblock Analyzing and improving the image quality of stylegan.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 8110--8119, 2020.

\bibitem{johnson2016perceptual}
Justin Johnson, Alexandre Alahi, and Li~Fei-Fei.
\newblock Perceptual losses for real-time style transfer and super-resolution.
\newblock In {\em European conference on computer vision}, pages 694--711.
  Springer, 2016.

\bibitem{zhang2018unreasonable}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual
  metric.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 586--595, 2018.

\bibitem{description2depiction}
Anonymized.
\newblock Anonymous paper under review.
\newblock 2022.

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{hutchinson2021towards}
Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina Greer, Oddur
  Kjartansson, Parker Barnes, and Margaret Mitchell.
\newblock Towards accountability for machine learning datasets: Practices from
  software engineering and infrastructure.
\newblock In {\em Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 560--575, 2021.

\bibitem{denton2021genealogy}
Emily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, and Hilary Nicole.
\newblock On the genealogy of machine learning datasets: A critical history of
  {ImageNet}.
\newblock {\em Big Data \& Society}, 8(2):20539517211035955, 2021.

\bibitem{mitchell2019model}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
  Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting.
\newblock In {\em Proceedings of the conference on fairness, accountability,
  and transparency}, pages 220--229, 2019.

\bibitem{gebru2021datasheets}
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer~Wortman Vaughan,
  Hanna Wallach, Hal~Daum{\'e} {III}, and Kate Crawford.
\newblock Datasheets for datasets.
\newblock {\em Communications of the ACM}, 64(12):86--92, 2021.

\bibitem{pushkarna2022data}
Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson.
\newblock {Data Cards}: Purposeful and transparent dataset documentation for
  responsible {AI}.
\newblock In {\em Proceedings of the 2022 ACM Conference on Fairness,
  Accountability, and Transparency}, 2022.

\bibitem{el2019tell}
Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla~El Asri,
  Samira~Ebrahimi Kahou, Yoshua Bengio, and Graham~W Taylor.
\newblock Tell, draw, and repeat: Generating and modifying images based on
  continual linguistic instruction.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10304--10312, 2019.

\bibitem{sharma2018chatpainter}
Shikhar Sharma, Dendi Suhubdy, Vincent Michalski, Samira Ebrahimi~Kahou, and
  Yoshua Bengio.
\newblock Chatpainter: Improving text to image generation using dialogue.
\newblock In {\em International Conference on Learning Representations:
  Workshop}, 2018.

\bibitem{mankoff2010disability}
Jennifer Mankoff, Gillian~R Hayes, and Devva Kasnitz.
\newblock Disability studies as a source of critical inquiry for the field of
  assistive technology.
\newblock In {\em Proceedings of the 12th international ACM SIGACCESS
  conference on Computers and accessibility}, pages 3--10, 2010.

\bibitem{browne_ai_artist}
Kieran Browne.
\newblock {Who (or What) Is an AI Artist?}
\newblock {\em Leonardo}, 55(2):130--134, 04 2022.

\bibitem{gen-art-bias}
Ramya Srinivasan and Kanji Uchino.
\newblock Biases in generative art: A causal look from the lens of art history.
\newblock In {\em Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, FAccT '21, page 41–51, New York, NY,
  USA, 2021. Association for Computing Machinery.

\bibitem{wu2016google}
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V Le, Mohammad Norouzi, Wolfgang
  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock {\em arXiv preprint arXiv:1609.08144}, 2016.

\bibitem{alva-manchego-etal-2020-data}
Fernando Alva-Manchego, Carolina Scarton, and Lucia Specia.
\newblock Data-driven sentence simplification: Survey and benchmark.
\newblock {\em Computational Linguistics}, 46(1):135--187, 2020.

\bibitem{zhou-bhat-2021-paraphrase}
Jianing Zhou and Suma Bhat.
\newblock Paraphrase generation: A survey of the state of the art.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 5075--5086, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{shankar2017no}
Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and
  D~Sculley.
\newblock No classification without representation: Assessing geodiversity
  issues in open data sets for the developing world.
\newblock {\em arXiv e-prints}, pages arXiv--1711, 2017.

\bibitem{de2019does}
Terrance De~Vries, Ishan Misra, Changhan Wang, and Laurens Van~der Maaten.
\newblock Does object recognition work for everyone?
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 52--59, 2019.

\bibitem{luo2020distortion}
Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, and Peyman Milanfar.
\newblock Distortion agnostic deep watermarking.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13548--13557, 2020.

\bibitem{birhane2021multimodal}
Abeba Birhane, Vinay~Uday Prabhu, and Emmanuel Kahembwe.
\newblock Multimodal datasets: misogyny, pornography, and malignant
  stereotypes.
\newblock {\em arXiv:2110.01963}, 2021.

\end{thebibliography}
