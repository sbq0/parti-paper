\section{Broader Impacts}
\label{secs:broader}

Beyond the model capabilities and evaluations presented above, there are broader issues to consider with large-scale models for text-to-image generation. Some of these issues pertain to the development process itself, including the use of large, mostly uncurated training datasets of images obtained from the web with little oversight (discussed also in~\cite{imagen}), or conceptual vagueness around constructs in the task formulation \cite{description2depiction}. Since large text-to-image models are foundation models \cite{bommasani2021opportunities} --- enabling both a range of system applications as well as finetuning for specific image generation tasks --- they act as a form of infrastructure which shapes our conceptions of what is both possible and desirable~\cite{hutchinson2021towards, denton2021genealogy}. Predicting all possible uses and consequences of infrastructure is difficult if not impossible, and so responsible AI practices which emphasize transparently documenting and sharing information about datasets and models are crucial~\cite{mitchell2019model, gebru2021datasheets, pushkarna2022data}. Although applications are beyond scope of this paper, we discuss here some likely opportunities and risks that can be anticipated.

\textbf{Creativity and art.} The ability of machine learned models to produce novel, high-quality images using language descriptions opens up many new possibilities for people to create unique and aesthetically appealing images, including artistic ones. Like a paint brush, these models are a kind of tool that on their own do not produce art---instead people use these tools to develop concepts and push their creative vision forward. For artists, such models could provide new means of innovation and exploration, including opportunities to create on-the-fly generation of art that sets a theme or style while responding to viewer interactions, or to generate novel and unique visual interactions in video game environments. For non-artists, these affordances present a chance to explore their visual creativity through a natural language interface which does not require technical artistic ability. Text-to-image systems could also assist creativity for people with disabilities (cf.~\cite{el2019tell, sharma2018chatpainter}), but we caution against doing so without also adopting participatory methods to increase the likelihood of actual needs being met and to avoid misconceptions about disability \cite{mankoff2010disability}.

Assessing the design merit or artistic merit (or lack thereof) of a piece created using machine learned models requires a nuanced understanding of algorithmically based art over the years, the model itself, the people involved and the broader artistic milieu \cite{browne_ai_artist}. The range of artistic outputs from a model is dependent on the training data, which may have cultural biases towards Western imagery, and which may prevent models from exhibiting radically new artistic styles the way human artists can \cite{gen-art-bias}.

\textbf{Visual (mis)communication.} The pre-ML history of text-to-image largely consists of assisting communication with non-literate groups including language learners (including children, \eg, storybook illustrations), low-literacy social groups (\eg, up until the late modern period, religious illustrations for low-literacy congregations), and speakers of other languages. \bdraw uses an architecture and strategy that is directly connected to the neural sequence-to-sequence models used for machine translation~\cite{wu2016google} and other communication aids such as sentence simplification \cite{alva-manchego-etal-2020-data} and paraphrasing \cite{zhou-bhat-2021-paraphrase}. This potentially strengthens the temptation to use large text-to-image models to assist with communication. However, we caution against the use of text-to-image models as communication aids, including for education (cf.~\cite{el2019tell}), until further research has examined questions of efficacy and utility, since text and image convey meaning in distinct ways and with distinct limitations. Cross-cultural considerations are of special concern, as little research has considered questions of accessibility of computer-generated images to members of non-Western cultures. Not only do visual styles differ cross-culturally, but also the form and appearance of instances of categories may radically differ across cultures (\eg, wedding attire, food, etc \cite{shankar2017no, de2019does}) in ways that might lead to miscommunication.

\textbf{Deepfakes and disinformation.} Given that the quality of model outputs is good enough to be confused for real photographs,\footnote{E.g., DALL-E 2 outputs: {\scriptsize \url{https://www.mattbell.us/my-fake-dall-e-2-vacation-photos-passed-the-turing-test/}}} and also because output quality and realism is rapidly improving, there are obvious concerns around using such technology to create deepfakes. One way to mitigate this problem is to apply watermarks that people cannot perceive to every generated image \cite{luo2020distortion}, such that it is possible to verify whether any given image is generated by a particular model such as \bdraw. While this approach may mitigate risks of disinformation, harms may still occur when an individual's likeness is reproduced without their consent.

\textbf{Bias and safety.} Text-to-image generation models like GLIDE, DALL-E 2, Imagen, Make-a-Scene, CogView and \bdraw are all trained on large, often noisy, image-text datasets that are known to contain biases regarding people of different backgrounds. This is particularly highlighted in Birhane et al's \cite{birhane2021multimodal} analysis of the LAION-400M dataset \cite{schuhmann2021laion}: their study of the dataset surfaced many problems with respect to stereotyping, pornography, violence and more. Other biases include stereotypical representations of people described as lawyers, flight attendants, homemakers, and so on. Models trained on such data without mitigation strategies thus risk reflecting and scaling up the underlying problems. Our primary training data is selected and highly filtered to minimize the presence of NSFW content; however, we incorporated LAION-400M during finetuning with classifier-free guidance -- this improved model performance but also led to generation of NSFW images in some contexts. Other biases include those introduced by the use of examples that primarily have English texts and may be biased to certain areas of the world. In informal testing, we have noticed, for example, that prompts mentioning wedding clothes seem to produce images biased towards stereotypically female and Western attire.

\textbf{Intended uses.} Due to the impacts and limitations described above, and the need for further exploration of concerns, \bdraw is a research prototype. It is not intended for use in high-risk or sensitive domains, and is not intended to be used for generating images of people.

These considerations all contribute to our decision not to release our models, code or data at this time. Instead, we will focus in follow-on work on further, careful measurement of model biases, along with mitigation strategies such as prompt filtering, output filtering and model recalibration. We also believe that it may be possible to use text-to-image generation models as tools to understand biases in large image-text datasets at scale, by explicitly probing them for a suite of known types of bias and also trying to uncover other forms of hidden bias.  We will also coordinate with artists to adapt capabilities of high performing text-to-image generation models toward their work, be it for purely creative ends or art-for-hire. This is all the more important given the intense interest among many research groups and the consequent fast pace of development of models and data for training them. Ideally, these models will augment---rather than replace---human creativity and productivity, such that we all can enjoy a world filled with new, varied and responsible aesthetic visual experiences.