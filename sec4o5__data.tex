\section{Training and Evaluation Datasets}
\subsection{Training Datasets}
\label{secs:data}

We train on a combination of image-text datasets for all \bdraw models. The data includes the publicly available LAION-400M dataset~\cite{schuhmann2021laion}; \webfit, a filtered subset of the full 1.8 billion examples used to train the ALIGN model~\cite{align-paper}; JFT-4B dataset~\cite{zhai2021scaling}, which has images with text annotation labels. For textual descriptions of JFT, we randomly switch between the original labels as text (concatenated if an image has multiple labels) or machine-generated captions from a SimVLM model~\cite{wang2021simvlm}. We discuss the limitations of the data in Section~\ref{secs:broader}. For all image inputs, we follow the DALL-E dVAE input processing (Section A.2. Training in~\cite{ramesh2021zero}) for image tokenizer training and the DALL-E Transformer input processing (Section B.2. Training in~\cite{ramesh2021zero}) for encoder-decoder training.

\subsection{Evaluation Datasets}
\input{tables/dataset_table}
We evaluate our models on MS-COCO (2014) \cite{lin2014microsoft} and Localized Narratives \cite{PontTuset_eccv2020}, summarized in Table~\ref{tabs:datasets}. MS-COCO is the current standard dataset for measuring both zero-shot and finetuned text-to-image generation performance, which makes it a consistent point of comparison with prior work. However, MS-COCO captions are short, high-level characterizations of their corresponding images. For a more comprehensive evaluation, we also use the COCO portion of Localized Narratives (LN-COCO), which provides longer, detailed descriptions of images corresponding to the MS-COCO (2017) dataset, and compare \bdraw's performance on LN-COCO against \cite{trecs2020, zhang2021cross}. These long-form descriptions are typically quite different from the descriptions used to train large text-to-image generation models. This provides a measure of generalization to out-of-domain distributions, as well as the finetuning capability of these models. Regardless of the community's current focus on zero-shot performance, the ability to finetune effectively is also important for adapting an open-domain text-to-image generation model to work with a specific application area or domain.

\subsection{\bcp{}}
\label{secs:bcp}

\input{tables/bcp_categories}

Existing benchmarks like MS-COCO \cite{lin2014microsoft} and Localized Narratives \cite{PontTuset_eccv2020} are clearly useful for measuring the progress of text-to-image synthesis systems, but the descriptions available in them are generally limited to everyday scenes and objects found in natural images. This limits their representation of a broad spectrum of prompts -- in particular, they lack prompts that allow us to better probe model capabilities on open-domain text-to-image generation. For example, MS-COCO captions are brief characterizations of high level participants and actions in images; these typically cover common scenarios and are oriented toward objects. Localized Narratives has highly-detailed descriptions, but also emphasizes natural scenes and objects. Recently, the work by \cite{park2021benchmark} focuses on the text-to-image generation task, but is limited to only two scenarios, unseen object-color (\eg, ``blue petal'') and object-shape (\eg, ``long beak'').
Motivated by these shortcomings, we present \bcp{} (\bcpa{}), a set of \bcpsize{} diverse English prompts that allow us to more comprehensively evaluate and test the limits of text-to-image synthesis models.

\input{tables/bcp_trickiness}


Each prompt in the \bcpa{} benchmark is associated with two labels: (1) {\it Category}, indicating a broad group that a prompt belongs to, and (2) {\it Challenge}, highlighting an aspect which makes a prompt difficult.
Table~\ref{t:bcp_categories} provides a few samples of categories (out of \bcpcat{} options) used in \bcpa{}, ranging from abstract concepts such as the ``golden ratio'' to concrete world-knowledge ones such as ``the skyline of New York City''. Similarly, Table~\ref{t:bcp_trickiness} lists a sample of challenge aspects (out of \bcptrick{}), ranging from basic ones such as ``a rabbit'' to complex ones such as a full description of the painting \textit{Starry Night} (``Oil-on-canvas painting of a blue night sky ... A church rises as a beacon against rolling blue hills.''). For example, the prompt ``a peaceful lakeside landscape with migrating herd of sauropods'' is categorized as \bcpstyle{Outdoor Scenes}, while its challenge aspect is \bcpstyle{Imagination}. Similarly, the prompt ``7 dogs sitting around a poker table, two of which are turning away'' has \bcpstyle{Animals} as category and \bcpstyle{Quantity} as challenge aspect. These two views of a prompt allows us to analyze a model's capabilities from both aspects--the overall content generated and the subtle details captured.

\begin{figure}[tbh!]
    \centering
    \includegraphics[width=1\textwidth]{figures/bcp.png}
    \caption{A summary of the \bcp{} (\bcpa{}) set of \bcpsize{} descriptions, spanning across many {\it category labels} (left) and {\it challenge aspects} (right).
    }
    \label{figs:bcp}
\end{figure}

We created \bcp{}
both by thinking of novel prompts and by manually curating and sampling prompts from recent papers~\cite{ramesh2021zero, ding2021cogview, vqdiffusion, nichol2021glide, ramesh2022hierarchical} (which accounts for about 7\% of prompts in \bcpa{}). While it is possible to assign multiple categories and challenge aspects to a prompt, we chose to reduce the complexity of model analysis by manually deciding on a single primary category and challenge aspect for each prompt. For example, when there is a proper noun, we will prefer to label the prompt as \bcpstyle{World Knowledge} (e.g., ``a painting of street in Paris'') over other categories, e.g., \bcpstyle{Arts}. We also prioritize categories that have fewer examples, \eg, \bcpstyle{Arts}, over those with plenty, such as \bcpstyle{Animals}, as in the case of the prompt ``A raccoon wearing formal clothes, wearing a tophat and holding a cane. The raccoon is holding a garbage bag. Oil painting in the style of Vincent Van Gogh.'' The \bcpstyle{People} category always takes priority, \eg, ``a team playing baseball at the beach'' is labeled as \bcpstyle{People} instead of \bcpstyle{Outdoor Scenes}; we do this to ease future work on fairness and bias interested in using \bcp, as it can be easily split to include or exclude prompts involving people.
Figure~\ref{figs:bcp} highlights the distribution of category labels and challenge aspects over the \bcpsize{} prompts. One can group these prompts into levels of difficulties by the challenge aspects: {\it Standard} includes \bcpstyle{Basic} and \bcpstyle{Simple Detail} (about 1/3 of the prompts); {\it Intermediate} includes \bcpstyle{Fine-grained Detail} and \bcpstyle{Style \& Format} (also about 1/3 of the prompts); and {\it Challenging} includes the remaining 7 challenge aspects such as \bcpstyle{Imagination}, \bcpstyle{Quantity}, \bcpstyle{Complex}, and \bcpstyle{Linguistic Structures}.

It is also worth mentioning DrawBench, a concurrently developed benchmark of 200 prompts introduced in \cite{imagen}. It has eleven labels that mix both categories (\eg, ``DALL-E'') and challenging aspects (\eg, ``Counting''). \bcp{}, in contrast, teases apart these two dimensions, with \bcpcat{} categories and \bcptrick{} challenging aspects, allowing for richer categorizations of prompts and more fined-grained analyses, together with \(8\times\) more prompts. Both benchmarks contain prompts that present strong challenges for current best models--including DALL-E 2, Imagen and \bdraw--and hopefully will inspire further benchmarks to increase the level of difficulty as future models continually improve.
