\section{Related Work}
\textbf{Text-to-image generation.} 
The task of text-to-image generation tackles the problem of synthesizing realistic images from natural language descriptions. Successful models enable many creative applications. WordsEye \cite{coyne2001wordseye} was a pioneering approach that was based on rule-based methods and explicit 3D representations. One of the earliest models based on deep learning~\cite{reed2016generative} proposed using conditional GANs for generating images of birds and flowers from language descriptions. Later works improved upon generation quality by introducing progressive refinement~\cite{Han17, Han17stackgan2} and using cross-modal attention mechanisms~\cite{Xu18, zhang2021cross}. Several other works propose using hierarchical models that generate images by explicitly modeling the location and semantics of objects~\cite{reed2016learning, HongYCL18, HinzHW19, trecs2020}.

Remarkable improvement has been achieved by treating text-to-image generation as a sequence modeling problem~\cite{Esser21vqgan, ding2021cogview, ramesh2021zero, gafni2022make, dallemini} trained on large-scale image-text pairs. A two-stage framework is usually exploited~\cite{yu2021vector, chang2022maskgit} where in the first stage the images are tokenized into discrete latent variables. With image tokenization and de-tokenization, text-to-image generation is treated as a sequence-to-sequence problem amenable to language models with transformers, which provide opportunities of scaling such models by applying techniques and observations from large language models~\cite{radford2018improving, du2021glam, chowdhery2022palm, hoffmann2022training}.

The most recent impressive results have been attained with diffusion models~\cite{nichol2021glide, ramesh2022hierarchical, imagen}, where the models are learned to condition on the text encoder of the CLIP~\cite{radford2021learning} image-text model, or frozen text encoder (like T5~\cite{raffel2019exploring}) pretrained by language self-supervision. Diffusion models work by positing a process of iteratively adding noise to an image and then learning to reverse that noise conditioned on text input or feature. When used with diffusion model cascading~\cite{ho2022cascaded}, these models have proven effective for generating high-fidelity images from text prompts and have achieved state-of-the-art zero-shot MS-COCO FID scores~\cite{imagen}.

\textbf{Image tokenizers.} Previous work has explored tokenizing images into discrete latent variables with a learned deep neural network. Early work like discrete Variational Auto-Encoders (dVAEs)~\cite{rolfe2016discrete} optimizes a probabilistic model with discrete latent variables to capture datasets composed of discrete classes. However, dVAEs often generate blurry pixels when applied to natural images. Recent work like VQGAN~\citep{Esser21vqgan} (based on VQVAE~\cite{van2017neural}) further applies adversarial loss~\cite{karras2020analyzing} and perceptual loss~\citep{johnson2016perceptual, zhang2018unreasonable} to synthesize images using convolutional neural networks with self-attention modules. ViT-VQGAN~\cite{yu2021vector} builds upon VQGAN, with improvements on both architecture and codebook learning. Transformers~\cite{vaswani2017attention} are used to encode images into latent variables and decode them back to images. We use ViT-VQGAN~\cite{yu2021vector} with slight modifications (see Section~\ref{secs:tokenizer}) as our image tokenizer.